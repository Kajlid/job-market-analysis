version: "3.8"

services:
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASS}
    volumes:
      - mongo_data:/data/db

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_name_dir=file:///hadoop/dfs/name
    ports:
      - "9000:9000"
      - "9870:9870"
    volumes:
      - hdfs_namenode:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_data_dir=file:///hadoop/dfs/data
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - hdfs_datanode:/hadoop/dfs/data

  spark:
    build: .
    container_name: spark-job-analysis
    depends_on:
      - mongodb
      - namenode
      - datanode
    volumes:
      - ./python_scripts:/app/python_scripts
      - ./output:/app/output
    env_file:
      - .env
    networks:
      - spark_hdfs_mongo_net

volumes:
  mongo_data:
  hdfs_namenode:
  hdfs_datanode:

networks:
  spark_hdfs_mongo_net:
    driver: bridge
